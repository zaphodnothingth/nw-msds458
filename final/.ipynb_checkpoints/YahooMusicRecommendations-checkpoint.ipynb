{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations, islice\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# parallelization\n",
    "import multiprocessing as mp\n",
    "import threading\n",
    "import queue\n",
    "global lck \n",
    "lck = threading.Lock()\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_gpickle(\"YahooMusic.gpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from neuralogic.nn import get_evaluator\n",
    "# from neuralogic.core import Backend\n",
    "# from neuralogic.core import Relation, R, Template, Var, V, Term\n",
    "# from neuralogic.core.settings import Settings, Optimizer\n",
    "# from neuralogic.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data source: https://www.kaggle.com/c/ee627a-2019fall/data?select=albumData2.txt\n",
    "# origin of data: https://webscope.sandbox.yahoo.com/catalog.php?datatype=c&guccounter=1&guce_referrer=aHR0cHM6Ly9naXRodWIuY29tL3NhcmFueWF2c3IvTXVzaWMtUmVjb21tZW5kYXRpb25z&guce_referrer_sig=AQAAADdDVj1NcJ7l9D0AF1OwjrIchcuyq2aDD8kc4qxRk3RP-B1mQTaY0IDliV2wsC-gQw05v-d9k8v70efaNULAbemXR_upER5MDVS8mcDsU_DQJZmtcUF8Sdh7A1holj3I-8UJVcKbI65keJp44o46CL8aGp2kLYhRCUYeTXkwxv9N\n",
    "\n",
    "\n",
    "data_list = glob.glob('ee627a-2019fall\\*')\n",
    "data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- trainItem2.txt - the training set \n",
    "- testItem2.txt - the test set \n",
    "- sample_ submission.csv - a sample submission file in the correct format \n",
    "- trackData2.txt -- Track information formatted as: <'TrackId'>|<'AlbumId'>|<'ArtistId'>|<'Optional GenreId_1'>|...|<'Optional GenreId_k'> \n",
    "- albumData2.txt -- Album information formatted as: <'AlbumId'>|<'ArtistId'>|<'Optional GenreId_1'>|...|<'Optional GenreId_k'> \n",
    "- artistData2.txt -- Artist listing formatted as: <'ArtistId'>\n",
    "- genreData2.txt -- Genre listing formatted as: <'GenreId'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# process tracks data file\n",
    "for row in open('ee627a-2019fall/trackData2.txt'):\n",
    "    row = row.strip('\\n').split('|')\n",
    "    G.add_node(row[0], attr = {\"node_type\": \"track\"}) # add track node\n",
    "    G.add_edge(row[0], row[1], weight=100) # connect track to album\n",
    "    G.add_node(row[1], attr = {\"node_type\": \"album\"}) # add album node\n",
    "    G.add_node(row[2], attr = {\"node_type\": \"artist\"}) # add artist node\n",
    "    if len(row) > 3:\n",
    "        for genre in row[3:]:\n",
    "            G.add_node(genre, attr = {\"node_type\": \"genre\"}) # add genre node\n",
    "            G.add_edge(row[0], genre, weight=100) # connect each genre to the track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process albums data file\n",
    "for row in open('ee627a-2019fall/albumData2.txt'):\n",
    "    row = row.strip('\\n').split('|')\n",
    "    G.add_edge(row[0], row[1], weight=100) # connect album to artist\n",
    "    if len(row) > 2:\n",
    "        for genre in row[2:]:\n",
    "            G.add_node(genre, attr = {\"node_type\": \"genre\"}) # add genre node\n",
    "            G.add_edge(row[0], genre, weight=100) # connect each genre to the album"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process training user data file\n",
    "for row in open('ee627a-2019fall/trainItem2.txt'):\n",
    "    if '|' in row:\n",
    "        cur_user = row.strip('\\n').split('|')[0] # pull user ID. don't need song count\n",
    "        G.add_node(cur_user, attr = {\"node_type\": \"user\"}) # add user node\n",
    "        continue # skip to the user's ratings\n",
    "    row = row.strip('\\n').split('\\t')\n",
    "    G.add_edge(cur_user, row[0], weight=int(row[1])) # connect user to song with rating as edge weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296101\n",
      "13342506\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(G.nodes))\n",
    "print(len(G.edges)) # 956820\n",
    "print(len([1 for cc in nx.connected_components(G)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gpickle(G, \"YahooMusic.gpickle\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G2 selects the largest subgraph. appears to just drop 4 disconnected nodes\n",
    "not currently needed because switched to astar, which is ok with weakly connected graphs  \n",
    "don't want to process if not necessary because the 2nd graph takes about 1.5GB of memory"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# G.remove_nodes_from(nx.isolates(G))\n",
    "\n",
    "largest_component = max(nx.connected_components(G), key=len)\n",
    "\n",
    "# Create a subgraph of G consisting only of this component:\n",
    "G2 = G.subgraph(largest_component)\n",
    "\n",
    "# print(len(combs = list(combinations(list(G.nodes), 2))))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(len(G2.nodes))\n",
    "print(len(G2.edges))\n",
    "print(len([1 for cc in nx.connected_components(G2)]))\n",
    "\n",
    "\"\"\"\n",
    "296097\n",
    "13342506\n",
    "1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing attempt 1. estimated time to completion of 20 days"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# process training user data file\n",
    "# astar is taking about 8-20sec per calc\n",
    "predictions = []\n",
    "with open('ee627a-2019fall/testItem2.txt') as f:\n",
    "    while True:\n",
    "        ratings = [] # reset for new user's list of songs\n",
    "        cur_set = [next(f).strip('\\n').split('|') for x in range(7)]\n",
    "        test_user = cur_set[0][0] # pull user ID. don't need song count  199810_208019\n",
    "        for song in cur_set[1:]:\n",
    "            dist=nx.astar_path_length(G, str(test_user), \\\n",
    "                                            str(song[0]), weight='weight') # distance from user to target song\n",
    "            ratings.append((f'{test_user}_{song}', dist))\n",
    "        ratings.sort(key=lambda x:x[1])\n",
    "        for i, j in enumerate(ratings):\n",
    "            cur_dict = {}\n",
    "            cur_dict['TrackID'] = f'{test_user}_{song}'\n",
    "            if i < 3:\n",
    "                cur_dict['Predictor'] = 1 # sorted ascending, smaller distance means closer & should be recommended\n",
    "            else:\n",
    "                cur_dict['Predictor'] = 0\n",
    "            predictions.append(cur_dict)\n",
    "\n",
    "\n",
    "# started at 0833\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)\n",
    "\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pickle\n",
    "\n",
    "with open('predictions.pkl', 'wb') as f:\n",
    "    pickle.dump(predictions, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing attempt 2. parallelized - ETC 6 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1strun = pd.read_csv('recommendations-Copy1.csv')\n",
    "df_1strun['TrackID'] = df_1strun['TrackID'].str.split(\"_\", n = 1, expand = True)\n",
    "\n",
    "completed_users = df_1strun.TrackID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_user(package, outfile='./recommendations.csv'):  \n",
    "    with open(outfile, 'a',encoding='utf-8-sig', newline='\\n') as g:\n",
    "        keys = package[0].keys() # get keys off first rating\n",
    "        dict_writer = csv.DictWriter(g, keys)\n",
    "        dict_writer.writerows(package)\n",
    "\n",
    "\n",
    "def process_user(i, infile='ee627a-2019fall/testItem2.txt'):\n",
    "    with open(infile, \"r\") as f:\n",
    "        lines_gen = islice(f, i*7, (i+1)*7) # get line index for userID & their last target song\n",
    "        cur_set = [x.strip('\\n').split('|') for x in lines_gen]\n",
    "    test_user = cur_set[0][0] # pull user ID. don't need song count  199810_208019\n",
    "    if test_user in completed_users:\n",
    "        return None\n",
    "    distances = []\n",
    "    for song in cur_set[1:]:\n",
    "        dist=nx.astar_path_length(G, str(test_user), \\\n",
    "                                     str(song[0]), \\\n",
    "                                     weight='weight') # distance from user to target song\n",
    "        distances.append((f'{test_user}_{song[0]}', dist))\n",
    "    distances.sort(key=lambda x:x[1])\n",
    "    predictions = []\n",
    "    for i, j in enumerate(distances):\n",
    "        cur_dict = {}\n",
    "        cur_dict['TrackID'] = j[0]\n",
    "        if i < 3:\n",
    "            cur_dict['Predictor'] = 1 # sorted ascending, smaller distance means closer & should be recommended\n",
    "        else:\n",
    "            cur_dict['Predictor'] = 0\n",
    "        predictions.append(cur_dict)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "class Worker(threading.Thread):\n",
    "\n",
    "    def __init__(self, q, i, *args, **kwargs):\n",
    "        self.q = q\n",
    "        self.i = i\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "            \n",
    "    def run(self):\n",
    "        while True:\n",
    "            try:\n",
    "                j = self.q.get(timeout=3)  # 3s timeout\n",
    "            except queue.Empty:\n",
    "                return\n",
    "            \n",
    "\n",
    "            predictions_processed = process_user(j)\n",
    "            if predictions_processed: # none will be returned if user already completed so we can skip\n",
    "                write_user(predictions_processed)\n",
    "\n",
    "            self.q.task_done()\n",
    "\n",
    "\n",
    "def mt_user_predictions(infile, outfile):\n",
    "    print('start time: {}'.format(datetime.now().strftime(\"%Y-%m-%d-%H.%M.%S\")))\n",
    "    start_time = time.time()\n",
    "\n",
    "    # n_threads= int(args[0])\n",
    "    n_threads = 30\n",
    "    ################### add desired output columns\n",
    "    with open(outfile ,'w') as oufl, open(infile, 'r', encoding='utf-8') as infl:\n",
    "        oufl.write('TrackID,Predictor')\n",
    "        row_count = sum(1 for row in infl)\n",
    "    print('total rows:', row_count) \n",
    "    \n",
    "    q = queue.Queue()\n",
    "    \n",
    "    n_users = int((row_count)/7)\n",
    "    with open(infile) as f:\n",
    "        for i in range(n_users):\n",
    "            q.put_nowait(i)\n",
    "    print(f'[{n_users}] users in target file. passing indices to user multithreading')\n",
    "    for _ in range(n_threads):\n",
    "        Worker(q, _).start()\n",
    "        time.sleep(1)\n",
    "    q.join()\n",
    "    \n",
    "    print('finished. end time: {}'.format(datetime.now().strftime(\"%Y-%m-%d-%H.%M.%S\")))\n",
    "    print('completed in {}'.format(timedelta(seconds=int(time.time() - start_time))))\n",
    "\n",
    "    ### multithreading doesn't help. this is PCU heavy. dumby....\n",
    "# mt_user_predictions(infile='ee627a-2019fall/testItem2.txt', outfile='./recommendations.csv')            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### switch to multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rows: 140000\n",
      "start time: 2022-03-15-18.12.22\n",
      "num workers avail: 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<multiprocessing.pool.IMapUnorderedIterator at 0x7fadc84fd450>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infile='ee627a-2019fall/testItem2.txt'\n",
    "outfile='./recommendations.csv'\n",
    "\n",
    "################### add desired output columns\n",
    "with open(outfile ,'w') as oufl, open(infile, 'r', encoding='utf-8') as infl:\n",
    "    oufl.write('TrackID,Predictor\\n')\n",
    "    row_count = sum(1 for row in infl)\n",
    "print('total rows:', row_count) \n",
    "n_users = int((row_count)/7)\n",
    "tasks = list(range(n_users))\n",
    "\n",
    "pbar = tqdm.tqdm(total=len(tasks))\n",
    "\n",
    "def run_mp(j):            \n",
    "    predictions_processed = process_user(j)\n",
    "    write_user(predictions_processed)\n",
    "    pbar.update(1)\n",
    " \n",
    "print('start time: {}'.format(datetime.now().strftime(\"%Y-%m-%d-%H.%M.%S\")))\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "num_workers = mp.cpu_count()\n",
    "print('num workers avail:', num_workers)\n",
    "\n",
    "pool = mp.Pool()\n",
    "pool.imap_unordered(run_mp, tasks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" try 1  \n",
    "pool = mp.Pool(num_workers)\n",
    "for _ in tqdm.tqdm(pool.map(func=run_mp, iterable=tasks, chunksize=20), total=len(tasks)):\n",
    "    pass\n",
    "\n",
    "result = pool.map(func=run_mp, iterable=tasks, chunksize=2)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" try 2\n",
    "# pool = mp.Pool()\n",
    "# pbar = tqdm.tqdm(total=len(tasks))\n",
    "\n",
    "# pool.imap_unordered(run_mp, tasks)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"try 3\n",
    "num_workers = mp.cpu_count()\n",
    "print('num workers avail:', num_workers)\n",
    "pbar = tqdm.tqdm(total=len(tasks))\n",
    "\n",
    "with mp.Pool(num_workers) as p:\n",
    "    for i, r in enumerate(p.imap_unordered(run_mp, tasks)):\n",
    "\n",
    "        pbar.update(i)\n",
    "\n",
    "pbar.close()\n",
    "\"\"\"\n",
    "#### at 12:27, 14hr eta means done before 3pm\n",
    "#### at 10:31, 1532125it [10:08:20, 146.29it/s], 10,500 lines written after 10 hrs. ~1000/hr = 140hrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker setup\n",
    "https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker\n",
    "\n",
    "start image:\n",
    "sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi\n",
    "alternate\n",
    "sudo nvidia-docker run -it -v ~/gits/nw-msds458/final:/wkg nvidia/cuda:11-base nvidia-smi\n",
    "\n",
    "------\n",
    "sudo docker run \\\n",
    "    --rm \\\n",
    "    -it \\\n",
    "    --gpus all \\\n",
    "    -v ~/gits/nw-msds458/final:/wkg \\\n",
    "    -e EXTRA_APT_PACKAGES=\"vim nano\" \\\n",
    "    -e EXTRA_CONDA_PACKAGES=\"jq\" \\\n",
    "    -e EXTRA_PIP_PACKAGES=\"networkx\" \\\n",
    "    -p 5555:8888 \\\n",
    "    -p 8787:8787 \\\n",
    "    -p 8786:8786 \\\n",
    "    rapidsai/rapidsai:22.02-cuda11.0-runtime-ubuntu18.04-py3.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graveyard  \n",
    "other distance measure attemps. even slower  \n",
    "also post processing that isn't necessary if i write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('fghj', 1), ('sdfg', 2), ('asdf', 3), ('hjkl', 23), ('ghjk', 34), ('dfgh', 80)]\n",
      "[{'TrackID': 'fghj', 'Predictor': 0}, {'TrackID': 'sdfg', 'Predictor': 0}, {'TrackID': 'asdf', 'Predictor': 0}, {'TrackID': 'hjkl', 'Predictor': 1}, {'TrackID': 'ghjk', 'Predictor': 1}, {'TrackID': 'dfgh', 'Predictor': 1}]\n"
     ]
    }
   ],
   "source": [
    "listy = [('asdf', 3), ('sdfg', 2), ('dfgh', 80), ('fghj', 1), ('ghjk', 34), ('hjkl', 23)]\n",
    "listy.sort(key=lambda x:x[1])\n",
    "print(listy)\n",
    "predicitons = []\n",
    "for i, j in enumerate(listy):\n",
    "    cur_dict = {}\n",
    "#     cur_dict['TrackID'] = f'{test_user}_{song}'\n",
    "    cur_dict['TrackID'] = f'{j[0]}'\n",
    "    if i < 3:\n",
    "        cur_dict['Predictor'] = 1 # sorted ascending, smaller distance means closer & should be recommended\n",
    "    else:\n",
    "        cur_dict['Predictor'] = 0\n",
    "    predicitons.append(cur_dict)\n",
    "print(predicitons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # connect all unconnected components with 0 weight so graph is strongly connected\n",
    "# combs = list(combinations(list(G.nodes), 2))\n",
    "# for comb in combs:\n",
    "#     if not G.has_edge(comb[0], comb[1]):\n",
    "#         G.add_edge(comb[0], comb[1], weight=0)\n",
    "\n",
    "### memory error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# too big\n",
    "# subax1 = plt.subplot(121)\n",
    "# nx.draw(G, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28 µs, sys: 1 µs, total: 29 µs\n",
      "Wall time: 31 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1', '214765']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time nx.shortest_path(G2, '1', '214765')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48 µs, sys: 1 µs, total: 49 µs\n",
      "Wall time: 51 µs\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    %time length=nx.astar_path_length(G, '1', '214765')\n",
    "except:\n",
    "    print('switching to subgraph')\n",
    "    %time length=nx.astar_path_length(G2, '1', '214765')\n",
    "\n",
    "# print(path['1']['214765'])\n",
    "print(length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time nx.resistance_distance(G2, '1', '214765')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %time length2=nx.dijkstra_path_length(G, '1', '214765')\n",
    "except:\n",
    "    print('switching to subgraph')\n",
    "    %time length2=nx.dijkstra_path_length(G2, '1', '214765')\n",
    "\n",
    "# print(path['1']['214765'])\n",
    "print(length2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time nx.dijkstra_path(G2, '1', '214765', weight='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     %time path=nx.all_pairs_dijkstra_path(G)\n",
    "# except:\n",
    "#     print('switching to subgraph')\n",
    "#     %time path=nx.all_pairs_dijkstra_path(G2)\n",
    "\n",
    "# print(path['1']['214765'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # is this cell necessary? process df in next for predicting 1/0\n",
    "# with f as open('ee627a-2019fall\\\\testItem2.txt'):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit to: https://www.kaggle.com/c/ee627a-2019fall/submit\n",
    "df_submission = pd.read_csv('ee627a-2019fall\\\\sample_submission.csv')\n",
    "df_submission[['user', 'track']] = df_submission['TrackID'].str.split('_', 1, expand=True)\n",
    "\n",
    "df_submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
